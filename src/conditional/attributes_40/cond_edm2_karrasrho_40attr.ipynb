{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5abdbb62",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-25T19:48:07.110782Z",
     "iopub.status.busy": "2025-11-25T19:48:07.110536Z",
     "iopub.status.idle": "2025-11-25T19:48:23.805588Z",
     "shell.execute_reply": "2025-11-25T19:48:23.804835Z"
    },
    "papermill": {
     "duration": 16.699931,
     "end_time": "2025-11-25T19:48:23.807272",
     "exception": false,
     "start_time": "2025-11-25T19:48:07.107341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (1.13.0)\r\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\r\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio) (1.26.4)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.3)\r\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.12.4)\r\n",
      "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.12.0.88)\r\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\r\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->imageio) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->imageio) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->imageio) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->imageio) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->imageio) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->imageio) (2.4.1)\r\n",
      "Collecting numpy (from imageio)\r\n",
      "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.41.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->imageio) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->imageio) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->imageio) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->imageio) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->imageio) (2024.2.0)\r\n",
      "INFO: pip is looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting mkl_fft (from numpy->imageio)\r\n",
      "  Downloading mkl_fft-2.1.1-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.3 kB)\r\n",
      "  Downloading mkl_fft-2.0.0-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.1 kB)\r\n",
      "INFO: pip is looking at multiple versions of mkl-random to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting mkl_random (from numpy->imageio)\r\n",
      "  Downloading mkl_random-1.3.0-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\r\n",
      "  Downloading mkl_random-1.2.11-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\r\n",
      "INFO: pip is looking at multiple versions of mkl-umath to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting mkl_umath (from numpy->imageio)\r\n",
      "  Downloading mkl_umath-0.3.0-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.3 kB)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->imageio) (2024.2.0)\r\n",
      "  Downloading mkl_umath-0.2.0-21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\r\n",
      "Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: numpy\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.26.4\r\n",
      "    Uninstalling numpy-1.26.4:\r\n",
      "      Successfully uninstalled numpy-1.26.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\r\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\r\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\r\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\r\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "ydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\r\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6\r\n",
      "Collecting numpy==1.26.4\r\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m217.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: numpy\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 2.2.6\r\n",
      "    Uninstalling numpy-2.2.6:\r\n",
      "      Successfully uninstalled numpy-2.2.6\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\r\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\r\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# üì¶ INSTALL REQUIRED DEPENDENCIES\n",
    "# -------------------------------------------------------------\n",
    "!pip install ninja imageio tqdm einops albumentations\n",
    "!pip install numpy==1.26.4 --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7744063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:48:23.816048Z",
     "iopub.status.busy": "2025-11-25T19:48:23.815253Z",
     "iopub.status.idle": "2025-11-25T19:48:25.089963Z",
     "shell.execute_reply": "2025-11-25T19:48:25.089258Z"
    },
    "papermill": {
     "duration": 1.280186,
     "end_time": "2025-11-25T19:48:25.091232",
     "exception": false,
     "start_time": "2025-11-25T19:48:23.811046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'edm2'...\r\n",
      "remote: Enumerating objects: 60, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\r\n",
      "remote: Total 60 (delta 13), reused 10 (delta 10), pack-reused 33 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (60/60), 1.27 MiB | 10.77 MiB/s, done.\r\n",
      "Resolving deltas: 100% (24/24), done.\r\n",
      "calculate_metrics.py  Dockerfile\t  README.md\t\ttrain_edm2.py\r\n",
      "count_flops.py\t      docs\t\t  reconstruct_phema.py\ttraining\r\n",
      "dataset_tool.py       generate_images.py  torch_utils\r\n",
      "dnnlib\t\t      LICENSE.txt\t  toy_example.py\r\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# üì• CLONE NVIDIA EDM2 REPOSITORY\n",
    "# -------------------------------------------------------------\n",
    "!git clone https://github.com/NVlabs/edm2.git\n",
    "!ls edm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1e3f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:48:25.099347Z",
     "iopub.status.busy": "2025-11-25T19:48:25.099101Z",
     "iopub.status.idle": "2025-11-25T19:48:27.319589Z",
     "shell.execute_reply": "2025-11-25T19:48:27.318664Z"
    },
    "papermill": {
     "duration": 2.226306,
     "end_time": "2025-11-25T19:48:27.321164",
     "exception": false,
     "start_time": "2025-11-25T19:48:25.094858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000001.jpg\r\n",
      "000002.jpg\r\n",
      "000003.jpg\r\n",
      "000004.jpg\r\n",
      "000005.jpg\r\n",
      "000006.jpg\r\n",
      "000007.jpg\r\n",
      "000008.jpg\r\n",
      "000009.jpg\r\n",
      "000010.jpg\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# üìÅ SET PATHS TO YOUR CELEBA64 DATASET\n",
    "# -------------------------------------------------------------\n",
    "DATA_ROOT = \"/kaggle/input/celeva-64x64-dataset/celeba64\"\n",
    "IMG_DIR   = f\"{DATA_ROOT}/train\"\n",
    "ATTR_CSV  = f\"{DATA_ROOT}/list_attr_celeba.csv\"\n",
    "\n",
    "# Show sample files\n",
    "!ls $IMG_DIR | head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e6f7ab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:48:27.329488Z",
     "iopub.status.busy": "2025-11-25T19:48:27.329246Z",
     "iopub.status.idle": "2025-11-25T19:48:31.830875Z",
     "shell.execute_reply": "2025-11-25T19:48:31.830175Z"
    },
    "papermill": {
     "duration": 4.507095,
     "end_time": "2025-11-25T19:48:31.832005",
     "exception": false,
     "start_time": "2025-11-25T19:48:27.324910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162770, 40)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# üß± BUILD ATTRIBUTE LABELS FROM list_attr_celeba.csv\n",
    "# -------------------------------------------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load attribute CSV\n",
    "attr_df = pd.read_csv(ATTR_CSV)\n",
    "\n",
    "# Extract image names and attributes\n",
    "image_ids_attr = attr_df.iloc[:, 0].values        # filenames\n",
    "attr_values = attr_df.iloc[:, 1:].values.astype(np.float32)  # 40 columns\n",
    "\n",
    "# Convert CelebA -1/+1 ‚Üí 0/1\n",
    "attr_values = (attr_values + 1) / 2.0\n",
    "\n",
    "# Load image filenames from the folder\n",
    "image_files = sorted([f for f in os.listdir(IMG_DIR) if f.lower().endswith(\".jpg\")])\n",
    "\n",
    "# Map: filename ‚Üí row index in CSV\n",
    "idx_map = {img_id: i for i, img_id in enumerate(image_ids_attr)}\n",
    "\n",
    "# Collect aligned labels\n",
    "labels_list = [attr_values[idx_map[fname]] for fname in image_files]\n",
    "\n",
    "labels = np.stack(labels_list, axis=0)\n",
    "\n",
    "labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c62988a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:48:31.840027Z",
     "iopub.status.busy": "2025-11-25T19:48:31.839823Z",
     "iopub.status.idle": "2025-11-25T19:48:31.905411Z",
     "shell.execute_reply": "2025-11-25T19:48:31.904598Z"
    },
    "papermill": {
     "duration": 0.070971,
     "end_time": "2025-11-25T19:48:31.906633",
     "exception": false,
     "start_time": "2025-11-25T19:48:31.835662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ['train_labels.npy', 'train_files.npy']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# üíæ SAVE LABEL FILES FOR DATASET\n",
    "# -------------------------------------------------------------\n",
    "SAVE_DIR = \"/kaggle/working/celeba64_labels\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "np.save(f\"{SAVE_DIR}/train_labels.npy\", labels)\n",
    "np.save(f\"{SAVE_DIR}/train_files.npy\", np.array(image_files))\n",
    "\n",
    "print(\"Saved:\", os.listdir(SAVE_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cd9fd6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:48:31.915446Z",
     "iopub.status.busy": "2025-11-25T19:48:31.915155Z",
     "iopub.status.idle": "2025-11-25T19:48:31.921590Z",
     "shell.execute_reply": "2025-11-25T19:48:31.920879Z"
    },
    "papermill": {
     "duration": 0.012209,
     "end_time": "2025-11-25T19:48:31.922650",
     "exception": false,
     "start_time": "2025-11-25T19:48:31.910441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting edm2/training/dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile edm2/training/dataset.py\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class ImageFolderDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Minimal EDM2-compatible dataset:\n",
    "      - loads images from `path`\n",
    "      - if `use_labels=True`, loads CelebA attributes from\n",
    "        `<parent_of_path>/list_attr_celeba.csv`\n",
    "      - exposes:\n",
    "          self.num_channels\n",
    "          self.has_labels\n",
    "          self.label_dim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, resolution=None, use_labels=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.use_labels = use_labels\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # 1) Collect image filenames\n",
    "        # -----------------------------------------------------\n",
    "        self._image_fnames = sorted(\n",
    "            f for f in os.listdir(path)\n",
    "            if f.lower().endswith((\".jpg\", \".png\"))\n",
    "        )\n",
    "        if len(self._image_fnames) == 0:\n",
    "            raise RuntimeError(f\"No images found in {path}\")\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # 2) Basic image info (num_channels, resolution)\n",
    "        # -----------------------------------------------------\n",
    "        first_img_path = os.path.join(self.path, self._image_fnames[0])\n",
    "        first_img = Image.open(first_img_path).convert(\"RGB\")\n",
    "        w, h = first_img.size\n",
    "        self.num_channels = 3\n",
    "        # EDM2 only uses img_resolution as a single int, and 64x64 is square.\n",
    "        self.resolution = h  # or w; they‚Äôre equal for CelebA-64\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # 3) Load CelebA attribute labels if available\n",
    "        # -----------------------------------------------------\n",
    "        self.labels = None\n",
    "        self.label_dim = 0\n",
    "        self.has_labels = False\n",
    "\n",
    "        if self.use_labels:\n",
    "            base = os.path.dirname(self.path)  # parent of `train`\n",
    "            attr_csv = os.path.join(base, \"list_attr_celeba.csv\")\n",
    "\n",
    "            if os.path.exists(attr_csv):\n",
    "                print(f\">> Loading CelebA attributes from: {attr_csv}\")\n",
    "                import pandas as pd\n",
    "\n",
    "                attr_df = pd.read_csv(attr_csv)\n",
    "\n",
    "                # Column 0: image_id, Columns 1‚Äì40: attributes\n",
    "                image_ids_attr = attr_df.iloc[:, 0].values\n",
    "                attr_values = attr_df.iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "                # Convert CelebA -1/+1 ‚Üí 0/1\n",
    "                attr_values = (attr_values + 1.0) / 2.0\n",
    "\n",
    "                # Map filename ‚Üí row index\n",
    "                idx_map = {img_id: i for i, img_id in enumerate(image_ids_attr)}\n",
    "\n",
    "                labels_list = []\n",
    "                missing = 0\n",
    "                for fname in self._image_fnames:\n",
    "                    if fname in idx_map:\n",
    "                        labels_list.append(attr_values[idx_map[fname]])\n",
    "                    else:\n",
    "                        # Fallback: all zeros if somehow missing\n",
    "                        missing += 1\n",
    "                        labels_list.append(np.zeros(attr_values.shape[1], dtype=np.float32))\n",
    "\n",
    "                if missing > 0:\n",
    "                    print(f\">> Warning: {missing} filenames not found in CSV; filled with zeros.\")\n",
    "\n",
    "                labels = np.stack(labels_list, axis=0)  # [N, 40]\n",
    "                self.labels = labels.astype(np.float32)\n",
    "                self.label_dim = self.labels.shape[1]\n",
    "                self.has_labels = True\n",
    "\n",
    "                print(f\">> Loaded labels with shape: {self.labels.shape}\")\n",
    "            else:\n",
    "                print(f\">> Attribute CSV not found at {attr_csv}. Running unconditional.\")\n",
    "        else:\n",
    "            print(\">> use_labels=False ‚Üí Unconditional mode.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._image_fnames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self._image_fnames[idx]\n",
    "        path = os.path.join(self.path, fname)\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # 4) Return raw pixels as uint8 [C,H,W]\n",
    "        #    EDM2 encoders expect uint8 in [0,255]\n",
    "        # -----------------------------------------------------\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = np.asarray(img, dtype=np.uint8)\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)  # [H,W,C] ‚Üí [C,H,W], uint8\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # 5) Return label vector\n",
    "        # -----------------------------------------------------\n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        else:\n",
    "            # Empty tensor if no labels\n",
    "            label = torch.zeros(0, dtype=torch.float32)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa75912d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:48:31.930710Z",
     "iopub.status.busy": "2025-11-25T19:48:31.930217Z",
     "iopub.status.idle": "2025-11-26T03:41:59.441446Z",
     "shell.execute_reply": "2025-11-26T03:41:59.440759Z"
    },
    "papermill": {
     "duration": 28407.516575,
     "end_time": "2025-11-26T03:41:59.442743",
     "exception": false,
     "start_time": "2025-11-25T19:48:31.926168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1125 19:48:38.474000 73 torch/distributed/run.py:792] \r\n",
      "W1125 19:48:38.474000 73 torch/distributed/run.py:792] *****************************************\r\n",
      "W1125 19:48:38.474000 73 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W1125 19:48:38.474000 73 torch/distributed/run.py:792] *****************************************\r\n",
      "[W1125 19:48:38.906150515 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1125 19:48:38.906796740 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1125 19:48:40.591616721 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1125 19:48:40.592487457 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1125 19:48:40.596328117 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1125 19:48:40.596935256 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "Setting up training config...\r\n",
      ">> Loading CelebA attributes from: /kaggle/input/celeva-64x64-dataset/celeba64/list_attr_celeba.csv\r\n",
      ">> Loading CelebA attributes from: /kaggle/input/celeva-64x64-dataset/celeba64/list_attr_celeba.csv\r\n",
      ">> Loaded labels with shape: (162770, 40)\r\n",
      ">> Loaded labels with shape: (162770, 40)\r\n",
      "\r\n",
      "Training config:\r\n",
      "{\r\n",
      "  \"dataset_kwargs\": {\r\n",
      "    \"class_name\": \"training.dataset.ImageFolderDataset\",\r\n",
      "    \"path\": \"/kaggle/input/celeva-64x64-dataset/celeba64/train\",\r\n",
      "    \"use_labels\": true\r\n",
      "  },\r\n",
      "  \"encoder_kwargs\": {\r\n",
      "    \"class_name\": \"training.encoders.StandardRGBEncoder\"\r\n",
      "  },\r\n",
      "  \"total_nimg\": 2097152,\r\n",
      "  \"batch_size\": 64,\r\n",
      "  \"network_kwargs\": {\r\n",
      "    \"class_name\": \"training.networks_edm2.Precond\",\r\n",
      "    \"model_channels\": 128,\r\n",
      "    \"dropout\": 0.0,\r\n",
      "    \"use_fp16\": true\r\n",
      "  },\r\n",
      "  \"loss_kwargs\": {\r\n",
      "    \"class_name\": \"training.training_loop.EDM2Loss\",\r\n",
      "    \"P_mean\": -0.8,\r\n",
      "    \"P_std\": 1.6\r\n",
      "  },\r\n",
      "  \"lr_kwargs\": {\r\n",
      "    \"func_name\": \"training.training_loop.learning_rate_schedule\",\r\n",
      "    \"ref_lr\": 0.012,\r\n",
      "    \"ref_batches\": 35000\r\n",
      "  },\r\n",
      "  \"batch_gpu\": 32,\r\n",
      "  \"loss_scaling\": 1.0,\r\n",
      "  \"cudnn_benchmark\": true,\r\n",
      "  \"status_nimg\": 16384,\r\n",
      "  \"snapshot_nimg\": 524288,\r\n",
      "  \"checkpoint_nimg\": null,\r\n",
      "  \"seed\": 0\r\n",
      "}\r\n",
      "\r\n",
      "Output directory:        /kaggle/working/training-runs/celeba64-cond-karras-rho\r\n",
      "Dataset path:            /kaggle/input/celeva-64x64-dataset/celeba64/train\r\n",
      "Class-conditional:       True\r\n",
      "Number of GPUs:          2\r\n",
      "Batch size:              64\r\n",
      "Mixed-precision:         True\r\n",
      "\r\n",
      "Creating output directory...\r\n",
      "[rank1]:[W1125 19:48:43.768078342 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\r\n",
      "[rank0]:[W1125 19:48:43.863151178 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\r\n",
      "Loading dataset...\r\n",
      ">> Loading CelebA attributes from: /kaggle/input/celeva-64x64-dataset/celeba64/list_attr_celeba.csv\r\n",
      ">> Loading CelebA attributes from: /kaggle/input/celeva-64x64-dataset/celeba64/list_attr_celeba.csv\r\n",
      ">> Loaded labels with shape: (162770, 40)\r\n",
      ">> Loaded labels with shape: (162770, 40)\r\n",
      "/kaggle/working/edm2/training/dataset.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\r\n",
      "  img = torch.from_numpy(img).permute(2, 0, 1)  # [H,W,C] ‚Üí [C,H,W], uint8\r\n",
      "/kaggle/working/edm2/training/dataset.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\r\n",
      "  img = torch.from_numpy(img).permute(2, 0, 1)  # [H,W,C] ‚Üí [C,H,W], uint8\r\n",
      "Setting up encoder...\r\n",
      "Constructing network...\r\n",
      "\r\n",
      "Precond                Parameters  Buffers  Output shape       Datatype\r\n",
      "---                    ---         ---      ---                ---     \r\n",
      "unet.emb_fourier       -           256      [32, 128]          float32 \r\n",
      "unet.emb_noise         65536       -        [32, 512]          float32 \r\n",
      "unet.emb_label         20480       -        [32, 512]          float32 \r\n",
      "unet.enc.64x64_conv    4608        -        [32, 128, 64, 64]  float16 \r\n",
      "unet.enc.64x64_block0  360449      -        [32, 128, 64, 64]  float16 \r\n",
      "unet.enc.64x64_block1  360449      -        [32, 128, 64, 64]  float16 \r\n",
      "unet.enc.64x64_block2  360449      -        [32, 128, 64, 64]  float16 \r\n",
      "unet.enc.32x32_down    360449      -        [32, 128, 32, 32]  float16 \r\n",
      "unet.enc.32x32_block0  1343489     -        [32, 256, 32, 32]  float16 \r\n",
      "unet.enc.32x32_block1  1310721     -        [32, 256, 32, 32]  float16 \r\n",
      "unet.enc.32x32_block2  1310721     -        [32, 256, 32, 32]  float16 \r\n",
      "unet.enc.16x16_down    1310721     -        [32, 256, 16, 16]  float16 \r\n",
      "unet.enc.16x16_block0  3538945     -        [32, 384, 16, 16]  float16 \r\n",
      "unet.enc.16x16_block1  3440641     -        [32, 384, 16, 16]  float16 \r\n",
      "unet.enc.16x16_block2  3440641     -        [32, 384, 16, 16]  float16 \r\n",
      "unet.enc.8x8_down      2850817     -        [32, 384, 8, 8]    float16 \r\n",
      "unet.enc.8x8_block0    6225921     -        [32, 512, 8, 8]    float16 \r\n",
      "unet.enc.8x8_block1    6029313     -        [32, 512, 8, 8]    float16 \r\n",
      "unet.enc.8x8_block2    6029313     -        [32, 512, 8, 8]    float16 \r\n",
      "unet.dec.8x8_in0       6029313     -        [32, 512, 8, 8]    float16 \r\n",
      "unet.dec.8x8_in1       4980737     -        [32, 512, 8, 8]    float16 \r\n",
      "unet.dec.8x8_block0    8912897     -        [32, 512, 8, 8]    float16 \r\n",
      "unet.dec.8x8_block1    8912897     -        [32, 512, 8, 8]    float16 \r\n",
      "unet.dec.8x8_block2    8912897     -        [32, 512, 8, 8]    float16 \r\n",
      "unet.dec.8x8_block3    8257537     -        [32, 512, 8, 8]    float16 \r\n",
      "unet.dec.16x16_up      4980737     -        [32, 512, 16, 16]  float16 \r\n",
      "unet.dec.16x16_block0  5554177     -        [32, 384, 16, 16]  float16 \r\n",
      "unet.dec.16x16_block1  5062657     -        [32, 384, 16, 16]  float16 \r\n",
      "unet.dec.16x16_block2  5062657     -        [32, 384, 16, 16]  float16 \r\n",
      "unet.dec.16x16_block3  4571137     -        [32, 384, 16, 16]  float16 \r\n",
      "unet.dec.32x32_up      2850817     -        [32, 384, 32, 32]  float16 \r\n",
      "unet.dec.32x32_block0  2359297     -        [32, 256, 32, 32]  float16 \r\n",
      "unet.dec.32x32_block1  2031617     -        [32, 256, 32, 32]  float16 \r\n",
      "unet.dec.32x32_block2  2031617     -        [32, 256, 32, 32]  float16 \r\n",
      "unet.dec.32x32_block3  1703937     -        [32, 256, 32, 32]  float16 \r\n",
      "unet.dec.64x64_up      1310721     -        [32, 256, 64, 64]  float16 \r\n",
      "unet.dec.64x64_block0  704513      -        [32, 128, 64, 64]  float16 \r\n",
      "unet.dec.64x64_block1  540673      -        [32, 128, 64, 64]  float16 \r\n",
      "unet.dec.64x64_block2  540673      -        [32, 128, 64, 64]  float16 \r\n",
      "unet.dec.64x64_block3  540673      -        [32, 128, 64, 64]  float16 \r\n",
      "unet.out_conv          3456        -        [32, 3, 64, 64]    float16 \r\n",
      "unet                   1           -        [32, 3, 64, 64]    float16 \r\n",
      "<top-level>            128         256      [32, 3, 64, 64]    float32 \r\n",
      "---                    ---         ---      ---                ---     \r\n",
      "Total                  124219429   512      -                  -       \r\n",
      "\r\n",
      "Setting up training state...\r\n",
      "Training from 0 kimg to 2097 kimg:\r\n",
      "\r\n",
      "Status: kimg 0.0       time 19s          sec/tick 18.95    sec/kimg 0.000   maintenance 18.95   cpumem 2.62   gpumem 1.87   reserved 2.30  \r\n",
      "/kaggle/working/edm2/training/dataset.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\r\n",
      "  img = torch.from_numpy(img).permute(2, 0, 1)  # [H,W,C] ‚Üí [C,H,W], uint8\r\n",
      "/kaggle/working/edm2/training/dataset.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\r\n",
      "  img = torch.from_numpy(img).permute(2, 0, 1)  # [H,W,C] ‚Üí [C,H,W], uint8\r\n",
      "/kaggle/working/edm2/training/dataset.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\r\n",
      "  img = torch.from_numpy(img).permute(2, 0, 1)  # [H,W,C] ‚Üí [C,H,W], uint8\r\n",
      "/kaggle/working/edm2/training/dataset.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\r\n",
      "  img = torch.from_numpy(img).permute(2, 0, 1)  # [H,W,C] ‚Üí [C,H,W], uint8\r\n",
      "Saving network-snapshot-0000000-0.050.pkl ... done\r\n",
      "Saving network-snapshot-0000000-0.100.pkl ... done\r\n",
      "Status: kimg 16.4      time 3m 57s       sec/tick 217.76   sec/kimg 13.207  maintenance 1.39    cpumem 3.31   gpumem 7.71   reserved 8.27  \r\n",
      "Status: kimg 32.8      time 7m 35s       sec/tick 218.24   sec/kimg 13.313  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 49.2      time 11m 14s      sec/tick 219.18   sec/kimg 13.370  maintenance 0.11    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 65.5      time 14m 54s      sec/tick 219.99   sec/kimg 13.420  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 81.9      time 18m 34s      sec/tick 220.29   sec/kimg 13.438  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 98.3      time 22m 15s      sec/tick 220.32   sec/kimg 13.440  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 114.7     time 25m 56s      sec/tick 220.94   sec/kimg 13.478  maintenance 0.11    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 131.1     time 29m 37s      sec/tick 221.34   sec/kimg 13.502  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 147.5     time 33m 19s      sec/tick 221.59   sec/kimg 13.518  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 163.8     time 37m 01s      sec/tick 221.92   sec/kimg 13.538  maintenance 0.11    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 180.2     time 40m 42s      sec/tick 221.87   sec/kimg 13.535  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 196.6     time 44m 24s      sec/tick 221.85   sec/kimg 13.533  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 213.0     time 48m 06s      sec/tick 222.16   sec/kimg 13.552  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 229.4     time 51m 48s      sec/tick 222.08   sec/kimg 13.547  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 245.8     time 55m 30s      sec/tick 221.95   sec/kimg 13.539  maintenance 0.11    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 262.1     time 59m 12s      sec/tick 222.03   sec/kimg 13.544  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 278.5     time 1h 02m 55s   sec/tick 222.21   sec/kimg 13.555  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 294.9     time 1h 06m 37s   sec/tick 222.25   sec/kimg 13.558  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 311.3     time 1h 10m 19s   sec/tick 222.15   sec/kimg 13.552  maintenance 0.11    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 327.7     time 1h 14m 01s   sec/tick 222.11   sec/kimg 13.550  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 344.1     time 1h 17m 43s   sec/tick 222.30   sec/kimg 13.561  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 360.4     time 1h 21m 26s   sec/tick 222.20   sec/kimg 13.555  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 376.8     time 1h 25m 08s   sec/tick 222.33   sec/kimg 13.562  maintenance 0.13    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 393.2     time 1h 28m 50s   sec/tick 222.16   sec/kimg 13.552  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 409.6     time 1h 32m 32s   sec/tick 222.20   sec/kimg 13.554  maintenance 0.13    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 426.0     time 1h 36m 15s   sec/tick 222.32   sec/kimg 13.562  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 442.4     time 1h 39m 57s   sec/tick 222.18   sec/kimg 13.553  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 458.8     time 1h 43m 39s   sec/tick 222.35   sec/kimg 13.564  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 475.1     time 1h 47m 21s   sec/tick 222.01   sec/kimg 13.544  maintenance 0.10    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 491.5     time 1h 51m 03s   sec/tick 222.13   sec/kimg 13.551  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 507.9     time 1h 54m 45s   sec/tick 221.93   sec/kimg 13.538  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 524.3     time 1h 58m 28s   sec/tick 222.29   sec/kimg 13.560  maintenance 0.12    cpumem 3.31   gpumem 7.70   reserved 8.27  \r\n",
      "Saving network-snapshot-0000524-0.050.pkl ... done\r\n",
      "Saving network-snapshot-0000524-0.100.pkl ... done\r\n",
      "Status: kimg 540.7     time 2h 02m 11s   sec/tick 223.04   sec/kimg 13.537  maintenance 1.25    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 557.1     time 2h 05m 53s   sec/tick 221.95   sec/kimg 13.540  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 573.4     time 2h 09m 35s   sec/tick 222.30   sec/kimg 13.561  maintenance 0.11    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 589.8     time 2h 13m 17s   sec/tick 221.78   sec/kimg 13.529  maintenance 0.11    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 606.2     time 2h 16m 59s   sec/tick 222.30   sec/kimg 13.561  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 622.6     time 2h 20m 41s   sec/tick 221.85   sec/kimg 13.534  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 639.0     time 2h 24m 23s   sec/tick 221.94   sec/kimg 13.539  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 655.4     time 2h 28m 05s   sec/tick 222.07   sec/kimg 13.547  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 671.7     time 2h 31m 47s   sec/tick 221.91   sec/kimg 13.537  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 688.1     time 2h 35m 29s   sec/tick 222.17   sec/kimg 13.553  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 704.5     time 2h 39m 11s   sec/tick 222.05   sec/kimg 13.545  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 720.9     time 2h 42m 53s   sec/tick 221.93   sec/kimg 13.538  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 737.3     time 2h 46m 35s   sec/tick 222.02   sec/kimg 13.544  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 753.7     time 2h 50m 17s   sec/tick 222.12   sec/kimg 13.550  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 770.0     time 2h 53m 59s   sec/tick 221.87   sec/kimg 13.534  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 786.4     time 2h 57m 41s   sec/tick 221.84   sec/kimg 13.533  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 802.8     time 3h 01m 23s   sec/tick 222.17   sec/kimg 13.552  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 819.2     time 3h 05m 05s   sec/tick 221.71   sec/kimg 13.525  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 835.6     time 3h 08m 46s   sec/tick 221.18   sec/kimg 13.493  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 852.0     time 3h 12m 27s   sec/tick 221.60   sec/kimg 13.518  maintenance 0.11    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 868.4     time 3h 16m 10s   sec/tick 222.32   sec/kimg 13.562  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 884.7     time 3h 19m 52s   sec/tick 222.08   sec/kimg 13.547  maintenance 0.13    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 901.1     time 3h 23m 33s   sec/tick 221.60   sec/kimg 13.518  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 917.5     time 3h 27m 15s   sec/tick 221.96   sec/kimg 13.540  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 933.9     time 3h 30m 57s   sec/tick 222.10   sec/kimg 13.549  maintenance 0.11    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 950.3     time 3h 34m 40s   sec/tick 222.15   sec/kimg 13.552  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 966.7     time 3h 38m 21s   sec/tick 221.51   sec/kimg 13.512  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 983.0     time 3h 42m 03s   sec/tick 221.81   sec/kimg 13.531  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 999.4     time 3h 45m 45s   sec/tick 221.95   sec/kimg 13.539  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1015.8    time 3h 49m 26s   sec/tick 221.56   sec/kimg 13.516  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1032.2    time 3h 53m 08s   sec/tick 221.66   sec/kimg 13.521  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1048.6    time 3h 56m 50s   sec/tick 221.81   sec/kimg 13.531  maintenance 0.11    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Saving network-snapshot-0001048-0.050.pkl ... done\r\n",
      "Saving network-snapshot-0001048-0.100.pkl ... done\r\n",
      "Status: kimg 1065.0    time 4h 00m 32s   sec/tick 222.43   sec/kimg 13.510  maintenance 1.08    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1081.3    time 4h 04m 14s   sec/tick 221.80   sec/kimg 13.530  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1097.7    time 4h 07m 56s   sec/tick 221.73   sec/kimg 13.526  maintenance 0.13    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1114.1    time 4h 11m 38s   sec/tick 221.97   sec/kimg 13.541  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1130.5    time 4h 15m 20s   sec/tick 222.08   sec/kimg 13.547  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1146.9    time 4h 19m 01s   sec/tick 221.50   sec/kimg 13.512  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1163.3    time 4h 22m 43s   sec/tick 221.90   sec/kimg 13.537  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1179.6    time 4h 26m 25s   sec/tick 221.75   sec/kimg 13.527  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1196.0    time 4h 30m 07s   sec/tick 221.70   sec/kimg 13.524  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1212.4    time 4h 33m 49s   sec/tick 221.80   sec/kimg 13.530  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1228.8    time 4h 37m 30s   sec/tick 221.63   sec/kimg 13.520  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1245.2    time 4h 41m 11s   sec/tick 221.29   sec/kimg 13.499  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1261.6    time 4h 44m 53s   sec/tick 221.46   sec/kimg 13.510  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1278.0    time 4h 48m 35s   sec/tick 221.81   sec/kimg 13.531  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1294.3    time 4h 52m 16s   sec/tick 221.48   sec/kimg 13.510  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1310.7    time 4h 55m 57s   sec/tick 221.12   sec/kimg 13.489  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1327.1    time 4h 59m 39s   sec/tick 221.47   sec/kimg 13.511  maintenance 0.11    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1343.5    time 5h 03m 21s   sec/tick 221.92   sec/kimg 13.538  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1359.9    time 5h 07m 03s   sec/tick 221.97   sec/kimg 13.541  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1376.3    time 5h 10m 45s   sec/tick 222.18   sec/kimg 13.553  maintenance 0.13    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1392.6    time 5h 14m 27s   sec/tick 221.78   sec/kimg 13.528  maintenance 0.13    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1409.0    time 5h 18m 08s   sec/tick 221.56   sec/kimg 13.515  maintenance 0.13    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1425.4    time 5h 21m 50s   sec/tick 221.55   sec/kimg 13.515  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1441.8    time 5h 25m 31s   sec/tick 221.72   sec/kimg 13.526  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1458.2    time 5h 29m 14s   sec/tick 222.10   sec/kimg 13.549  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1474.6    time 5h 32m 55s   sec/tick 221.50   sec/kimg 13.512  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1490.9    time 5h 36m 36s   sec/tick 221.32   sec/kimg 13.501  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1507.3    time 5h 40m 18s   sec/tick 221.54   sec/kimg 13.514  maintenance 0.13    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1523.7    time 5h 44m 00s   sec/tick 221.73   sec/kimg 13.526  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1540.1    time 5h 47m 41s   sec/tick 221.67   sec/kimg 13.523  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1556.5    time 5h 51m 23s   sec/tick 221.66   sec/kimg 13.522  maintenance 0.12    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1572.9    time 5h 55m 04s   sec/tick 221.36   sec/kimg 13.503  maintenance 0.13    cpumem 3.30   gpumem 7.70   reserved 8.27  \r\n",
      "Saving network-snapshot-0001572-0.050.pkl ... done\r\n",
      "Saving network-snapshot-0001572-0.100.pkl ... done\r\n",
      "Status: kimg 1589.2    time 5h 58m 47s   sec/tick 222.35   sec/kimg 13.505  maintenance 1.08    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1605.6    time 6h 02m 29s   sec/tick 221.85   sec/kimg 13.534  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1622.0    time 6h 06m 10s   sec/tick 221.30   sec/kimg 13.500  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1638.4    time 6h 09m 52s   sec/tick 221.73   sec/kimg 13.526  maintenance 0.11    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1654.8    time 6h 13m 33s   sec/tick 221.39   sec/kimg 13.506  maintenance 0.11    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1671.2    time 6h 17m 14s   sec/tick 221.02   sec/kimg 13.483  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1687.6    time 6h 20m 55s   sec/tick 221.00   sec/kimg 13.482  maintenance 0.11    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1703.9    time 6h 24m 36s   sec/tick 221.06   sec/kimg 13.485  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1720.3    time 6h 28m 17s   sec/tick 221.28   sec/kimg 13.499  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1736.7    time 6h 31m 59s   sec/tick 221.83   sec/kimg 13.532  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1753.1    time 6h 35m 41s   sec/tick 221.54   sec/kimg 13.514  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1769.5    time 6h 39m 22s   sec/tick 221.61   sec/kimg 13.519  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1785.9    time 6h 43m 04s   sec/tick 221.46   sec/kimg 13.510  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1802.2    time 6h 46m 45s   sec/tick 221.08   sec/kimg 13.487  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1818.6    time 6h 50m 26s   sec/tick 221.25   sec/kimg 13.497  maintenance 0.11    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1835.0    time 6h 54m 07s   sec/tick 221.25   sec/kimg 13.497  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1851.4    time 6h 57m 49s   sec/tick 221.29   sec/kimg 13.499  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1867.8    time 7h 01m 30s   sec/tick 221.40   sec/kimg 13.506  maintenance 0.11    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1884.2    time 7h 05m 11s   sec/tick 221.40   sec/kimg 13.506  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1900.5    time 7h 08m 53s   sec/tick 221.31   sec/kimg 13.500  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1916.9    time 7h 12m 34s   sec/tick 221.40   sec/kimg 13.506  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1933.3    time 7h 16m 15s   sec/tick 221.20   sec/kimg 13.493  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1949.7    time 7h 19m 57s   sec/tick 221.43   sec/kimg 13.508  maintenance 0.11    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1966.1    time 7h 23m 38s   sec/tick 221.48   sec/kimg 13.511  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1982.5    time 7h 27m 20s   sec/tick 221.86   sec/kimg 13.534  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 1998.8    time 7h 31m 01s   sec/tick 221.39   sec/kimg 13.505  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 2015.2    time 7h 34m 43s   sec/tick 221.66   sec/kimg 13.522  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 2031.6    time 7h 38m 25s   sec/tick 221.53   sec/kimg 13.514  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 2048.0    time 7h 42m 06s   sec/tick 221.67   sec/kimg 13.522  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 2064.4    time 7h 45m 48s   sec/tick 221.89   sec/kimg 13.536  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 2080.8    time 7h 49m 30s   sec/tick 221.81   sec/kimg 13.531  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Status: kimg 2097.2    time 7h 53m 12s   sec/tick 221.56   sec/kimg 13.516  maintenance 0.12    cpumem 3.35   gpumem 7.70   reserved 8.27  \r\n",
      "Saving network-snapshot-0002097-0.050.pkl ... done\r\n",
      "Saving network-snapshot-0002097-0.100.pkl ... done\r\n",
      "[rank0]:[W1126 03:41:57.123678512 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 edm2/train_edm2.py \\\n",
    "    --outdir=/kaggle/working/training-runs/celeba64-cond-karras-rho \\\n",
    "    --data=/kaggle/input/celeva-64x64-dataset/celeba64/train \\\n",
    "    --cond=True \\\n",
    "    --preset=edm2-img64-xs \\\n",
    "    --batch=64 \\\n",
    "    --batch-gpu=32 \\\n",
    "    --duration=2Mi \\\n",
    "    --status=16Ki \\\n",
    "    --snapshot=512Ki \\\n",
    "    --checkpoint=0 \\\n",
    "    --seed=0"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8643764,
     "sourceId": 13602561,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28437.900972,
   "end_time": "2025-11-26T03:41:59.871200",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-25T19:48:01.970228",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
