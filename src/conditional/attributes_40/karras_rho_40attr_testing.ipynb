{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fca85cd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-01T14:13:29.650374Z",
     "iopub.status.busy": "2025-12-01T14:13:29.649700Z",
     "iopub.status.idle": "2025-12-01T14:13:37.954997Z",
     "shell.execute_reply": "2025-12-01T14:13:37.954087Z"
    },
    "papermill": {
     "duration": 8.310159,
     "end_time": "2025-12-01T14:13:37.956280",
     "exception": false,
     "start_time": "2025-12-01T14:13:29.646121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'edm2'...\r\n",
      "remote: Enumerating objects: 60, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\r\n",
      "remote: Total 60 (delta 13), reused 10 (delta 10), pack-reused 33 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (60/60), 1.27 MiB | 10.86 MiB/s, done.\r\n",
      "Resolving deltas: 100% (24/24), done.\r\n",
      "/kaggle/working/edm2\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\r\n",
      "\u001b[0m/kaggle/working\n",
      "EDM2 setup complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ“¦ BLOCK 1 â€” Install Dependencies & Clone EDM2 Repository\n",
    "# ============================================================\n",
    "\n",
    "!pip install click tqdm psutil scipy pillow pandas --quiet\n",
    "\n",
    "# Clone EDM2 repo\n",
    "!git clone https://github.com/NVlabs/edm2.git\n",
    "%cd edm2\n",
    "\n",
    "# Install any remaining minimal dependencies\n",
    "!pip install -r requirements.txt --quiet || true\n",
    "\n",
    "# Return to working directory\n",
    "%cd /kaggle/working\n",
    "\n",
    "print(\"EDM2 setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e85eccd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T14:13:37.962943Z",
     "iopub.status.busy": "2025-12-01T14:13:37.962286Z",
     "iopub.status.idle": "2025-12-01T14:16:47.667828Z",
     "shell.execute_reply": "2025-12-01T14:16:47.667054Z"
    },
    "papermill": {
     "duration": 189.711914,
     "end_time": "2025-12-01T14:16:47.670961",
     "exception": false,
     "start_time": "2025-12-01T14:13:37.959047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test images: 19962\n",
      "Valid test images (in CSV): 19962\n",
      "Saved FULL test labels:\n",
      "/kaggle/working/cond-test-labels/labels_full.npy\n",
      "/kaggle/working/cond-test-labels/filenames_full.npy\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ§© BLOCK 2 â€” Prepare FULL Test Labels + Filenames\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "ATTR_CSV = \"/kaggle/input/celeva-64x64-dataset/celeba64/list_attr_celeba.csv\"\n",
    "TEST_DIR = \"/kaggle/input/celeva-64x64-dataset/celeba64/test\"\n",
    "OUT_DIR = \"/kaggle/working/cond-test-labels\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load attributes\n",
    "df = pd.read_csv(ATTR_CSV)\n",
    "img_col = df.columns[0]\n",
    "attr_cols = df.columns[1:41]\n",
    "\n",
    "# Convert -1/+1 â†’ 0/1\n",
    "attrs = (df[attr_cols].values > 0).astype(np.float32)\n",
    "\n",
    "# All test images\n",
    "test_files = sorted([f for f in os.listdir(TEST_DIR) if f.endswith(\".jpg\")])\n",
    "print(\"Total test images:\", len(test_files))\n",
    "\n",
    "# Only keep images present in CSV\n",
    "valid_test_files = [f for f in test_files if f in df[img_col].values]\n",
    "print(\"Valid test images (in CSV):\", len(valid_test_files))\n",
    "\n",
    "# Build full array (NO random sampling)\n",
    "labels_full = np.array([\n",
    "    attrs[df[img_col].values.tolist().index(f)]\n",
    "    for f in valid_test_files\n",
    "])\n",
    "filenames_full = np.array(valid_test_files)\n",
    "\n",
    "# Save\n",
    "np.save(f\"{OUT_DIR}/labels_full.npy\", labels_full)\n",
    "np.save(f\"{OUT_DIR}/filenames_full.npy\", filenames_full)\n",
    "\n",
    "print(\"Saved FULL test labels:\")\n",
    "print(f\"{OUT_DIR}/labels_full.npy\")\n",
    "print(f\"{OUT_DIR}/filenames_full.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c2f205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T14:16:47.676921Z",
     "iopub.status.busy": "2025-12-01T14:16:47.676321Z",
     "iopub.status.idle": "2025-12-01T14:16:47.682053Z",
     "shell.execute_reply": "2025-12-01T14:16:47.681427Z"
    },
    "papermill": {
     "duration": 0.009816,
     "end_time": "2025-12-01T14:16:47.683038",
     "exception": false,
     "start_time": "2025-12-01T14:16:47.673222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… gen_conditional.py written successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸŽ¨ BLOCK 3 â€” Write Conditional Generator Script (NO %%writefile)\n",
    "# ============================================================\n",
    "\n",
    "generator_code = r'''\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import sys\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Make sure EDM2 repo is importable\n",
    "# ------------------------------------------------------------\n",
    "sys.path.append(\"/kaggle/working/edm2\")\n",
    "import dnnlib\n",
    "from edm2.generate_images import edm_sampler\n",
    "\n",
    "\n",
    "def generate_conditional(model, labels, names, outdir,\n",
    "                         steps=32, sigma_min=0.002, sigma_max=80, rho=7):\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(\"Loading model:\", model)\n",
    "    with open(model, \"rb\") as f:\n",
    "        net = pickle.load(f)[\"ema\"].to(device).eval()\n",
    "\n",
    "    labels_np = np.load(labels)\n",
    "    filenames = np.load(names)\n",
    "\n",
    "    N = labels_np.shape[0]\n",
    "    print(f\"Generating {N} images...\")\n",
    "\n",
    "    bs_max = 64\n",
    "\n",
    "    for start in range(0, N, bs_max):\n",
    "        bs = min(bs_max, N - start)\n",
    "        cond = torch.tensor(labels_np[start:start+bs], device=device)\n",
    "        noise = torch.randn(bs, net.img_channels, net.img_resolution, net.img_resolution, device=device)\n",
    "\n",
    "        # EDM2 sampler (Karras rho)\n",
    "        imgs = edm_sampler(\n",
    "            net=net,\n",
    "            noise=noise,\n",
    "            labels=cond,\n",
    "            num_steps=steps,\n",
    "            sigma_min=sigma_min,\n",
    "            sigma_max=sigma_max,\n",
    "            rho=rho,\n",
    "        )\n",
    "\n",
    "        imgs = imgs.clamp(-1, 1)\n",
    "        imgs = (imgs * 127.5 + 127.5).to(torch.uint8)\n",
    "        imgs = imgs.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "        for j in range(bs):\n",
    "            outname = filenames[start + j].replace(\".jpg\", \".png\")\n",
    "            PIL.Image.fromarray(imgs[j]).save(os.path.join(outdir, outname))\n",
    "\n",
    "    print(\"DONE! Saved images to\", outdir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--model\", type=str, required=True)\n",
    "    parser.add_argument(\"--labels\", type=str, required=True)\n",
    "    parser.add_argument(\"--names\", type=str, required=True)\n",
    "    parser.add_argument(\"--outdir\", type=str, required=True)\n",
    "    parser.add_argument(\"--steps\", type=int, default=32)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    generate_conditional(\n",
    "        model=args.model,\n",
    "        labels=args.labels,\n",
    "        names=args.names,\n",
    "        outdir=args.outdir,\n",
    "        steps=args.steps,\n",
    "    )\n",
    "'''\n",
    "\n",
    "# Write generator script to a file\n",
    "with open(\"gen_conditional.py\", \"w\") as f:\n",
    "    f.write(generator_code)\n",
    "\n",
    "print(\"âœ… gen_conditional.py written successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e5915c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T14:16:47.688431Z",
     "iopub.status.busy": "2025-12-01T14:16:47.687969Z",
     "iopub.status.idle": "2025-12-01T17:12:46.294600Z",
     "shell.execute_reply": "2025-12-01T17:12:46.293848Z"
    },
    "papermill": {
     "duration": 10558.610756,
     "end_time": "2025-12-01T17:12:46.296018",
     "exception": false,
     "start_time": "2025-12-01T14:16:47.685262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /kaggle/input/network-snapshot-0001572-0-100/pytorch/default/1/network-snapshot-0001572-0.100.pkl\r\n",
      "Generating 19962 images...\r\n",
      "DONE! Saved images to /kaggle/working/cond-generated-full\r\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸš€ BLOCK 4 â€” Run Conditional Generation (FULL test set)\n",
    "# ============================================================\n",
    "\n",
    "MODEL_PKL = \"/kaggle/input/network-snapshot-0001572-0-100/pytorch/default/1/network-snapshot-0001572-0.100.pkl\"\n",
    "\n",
    "LABELS_FULL = \"/kaggle/working/cond-test-labels/labels_full.npy\"\n",
    "FILENAMES_FULL = \"/kaggle/working/cond-test-labels/filenames_full.npy\"\n",
    "\n",
    "OUT_IMAGES = \"/kaggle/working/cond-generated-full\"\n",
    "\n",
    "!python gen_conditional.py \\\n",
    "    --model \"$MODEL_PKL\" \\\n",
    "    --labels \"$LABELS_FULL\" \\\n",
    "    --names \"$FILENAMES_FULL\" \\\n",
    "    --outdir \"$OUT_IMAGES\" \\\n",
    "    --steps 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb2de3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T17:12:46.302013Z",
     "iopub.status.busy": "2025-12-01T17:12:46.301737Z",
     "iopub.status.idle": "2025-12-01T17:15:16.355900Z",
     "shell.execute_reply": "2025-12-01T17:15:16.355227Z"
    },
    "papermill": {
     "duration": 150.058825,
     "end_time": "2025-12-01T17:15:16.357279",
     "exception": false,
     "start_time": "2025-12-01T17:12:46.298454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from /kaggle/input/celeva-64x64-dataset/celeba64/test ...\r\n",
      "[rank0]:[W1201 17:13:37.719583878 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\r\n",
      "Setting up InceptionV3Detector...\r\n",
      "Calculating feature statistics...\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 312/312 [01:36<00:00,  3.24batch/s]\r\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š BLOCK 5 â€” Compute CelebA64 Reference Stats (once)\n",
    "# ============================================================\n",
    "\n",
    "# Compute reference statistics on whole test set (19,962 images)\n",
    "!python /kaggle/working/edm2/calculate_metrics.py ref \\\n",
    "    --data=\"/kaggle/input/celeva-64x64-dataset/celeba64/test\" \\\n",
    "    --dest=\"/kaggle/working/celeba64_ref.pkl\" \\\n",
    "    --metrics=fid \\\n",
    "    --batch=64 \\\n",
    "    --workers=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b5e79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T17:15:16.387968Z",
     "iopub.status.busy": "2025-12-01T17:15:16.387405Z",
     "iopub.status.idle": "2025-12-01T17:16:16.932026Z",
     "shell.execute_reply": "2025-12-01T17:16:16.931268Z"
    },
    "papermill": {
     "duration": 60.561833,
     "end_time": "2025-12-01T17:16:16.933473",
     "exception": false,
     "start_time": "2025-12-01T17:15:16.371640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1201 17:15:18.235000 101 torch/distributed/run.py:792] \r\n",
      "W1201 17:15:18.235000 101 torch/distributed/run.py:792] *****************************************\r\n",
      "W1201 17:15:18.235000 101 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W1201 17:15:18.235000 101 torch/distributed/run.py:792] *****************************************\r\n",
      "[W1201 17:15:18.722062623 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1201 17:15:18.722739527 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1201 17:15:20.577236791 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1201 17:15:20.577977499 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1201 17:15:20.598248535 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1201 17:15:20.598912317 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "Loading feature statistics from /kaggle/working/celeba64_ref.pkl ...\r\n",
      "Loading images from /kaggle/working/cond-generated-full ...\r\n",
      "[rank1]:[W1201 17:15:20.770857085 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\r\n",
      "[rank0]:[W1201 17:15:20.929849182 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\r\n",
      "Setting up InceptionV3Detector...\r\n",
      "Calculating feature statistics...\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [00:48<00:00,  3.22batch/s]\r\n",
      "Calculating fid...\r\n",
      "fid = 20.2637\r\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ§® BLOCK 6 â€” Compute FID for FULL test set (2 GPUs)\n",
    "# ============================================================\n",
    "\n",
    "NUM_FULL = len(os.listdir(\"/kaggle/working/cond-generated-full\"))\n",
    "\n",
    "!torchrun --standalone --nproc_per_node=2 \\\n",
    "    /kaggle/working/edm2/calculate_metrics.py calc \\\n",
    "    --images=\"/kaggle/working/cond-generated-full\" \\\n",
    "    --ref=\"/kaggle/working/celeba64_ref.pkl\" \\\n",
    "    --metrics=fid \\\n",
    "    --num=$NUM_FULL \\\n",
    "    --batch=64 \\\n",
    "    --workers=2\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8643764,
     "sourceId": 13602561,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 520313,
     "modelInstanceId": 505458,
     "sourceId": 667655,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10972.945017,
   "end_time": "2025-12-01T17:16:17.372639",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-01T14:13:24.427622",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
